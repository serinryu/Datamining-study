# 1장. 빅데이터 기초 지식

## 빅데이터의 취급이 어려운 이유

1. 데이터 분석 방법을 모름
2. 데이터 처리에 수고와 시간이 걸림. → 이 책에서 다룰것. 데이터 분석 지식이 있더라도 효율적인 데이터 처리가 이뤄지지 않으면 할 수 있는 일은 적어짐. 어떻게 하면 효율적인 처리를 가능하게 할까? (데이터 엔지니어의 역할)

## 빅데이터 기술의 시작

인터넷의 보급으로 세계 곳곳에서 액세스 되는 시스템이 증가 → 기존 전통적인 관계형 데이터베이스(RDB)로는 취급할 수 없을 만큼 대량의 데이터 축적. 기존과 다른 구조에 대한 니즈 증가.

1. Hadoop : 다수의 컴퓨터에서 대량의 데이터를 처리하기 위한 시스템. 모여진 데이터를 나중에 집계하는 것이 목적.
2. NoSQL 데이터베이스 : RDB보다 고속의 읽기, 쓰기 가능 및 분산 처리에 효과적. 모여진 데이터를 집계하는 목적인 하둡과는 달리, 애플리케이션에서 온라인으로 접속하는 데이터베이스 
    - 주요 NoSQL 데이터 : MongoDB, CouchDB, Riak, Cassandra, Redis

→ 이 둘을 조합해서 ‘NoSQL 데이터베이스에 기록하고 Haddop으로 분산처리하기'라는 흐름이 정착하게 됨. (분산 시스템에 의한 데이터 처리의 고속화) (2011년)

→ 분산 시스템의 비지니스 이용 개척 : 데이터 웨어하우스(데이터 분석을 기반) 제품이 사용되는 경우에도 하둡을 사용하는 경우가 증가. 가속도적으로 늘어나는 데이터 처리는 하둡에 맡기고, 비교적 작거나 중요한 데이터만을 데이터 웨어하우스에 넣는 식으로 사용을 구분하면서 데이터 웨어하우스의 부화를 줄일 수 있게 됨. 정리하자면, 하둡과 NoSQL 의 데이터베이스 분산 시스템 기술의 확립이 기존의 데이터 웨어 하우스를 보완, 대체하기 시작.

## 빅데이터 시대의 데이터 분석 기반

빅데이터 기술이 기존의 데이터 웨어하우스와 다른 점은 **다수의 분산 시스템**을 조합하여 확장성이 뛰어난 데이터 처리 구조를 만든다는 것. 정리하자면, ‘빅데이터 기술'이란 분산 시스템을 활용하면서 데이터를 순차적으로 가공해나가는 일련의 구조. (반대로, 스몰 데이터는 분산 시스템을 전혀 사용하지 않는 한 대의 컴퓨터로 데이터를 탐색하는 것. 해결하고자 하는 문제가 스몰 데이터 기술로 충분하다면, 그것을 사용하는 것이 시간을 훨씬 절약할 수 있다.)

- 데이터 파이프라인 : 일반적으로 차례대로 전달해나가는 데이터로 구성된 시스템 (데이터 수집 ~ 워크플로 관리). 어디서 데이터를 수집하여 무엇을 실현하고 싶은지에 따라 변화.

1. 데이터 수집 : 데이터 파이프라인의 시작점. 데이터는 여러 장소에서 발생하고, 서로 다른 기술로 데이터를 전송함. 데이터 전송(data transfer)방법에는 크게 1) 벌크(bulk)형, 2) 스트리밍(streaming)형. 
    - 벌크형 : 이미 어딘가에 존재하는 데이터를 정리해 추출하는 방법. (데이터베이스와 파일 서버 등에서 정기적으로 데이터를 수집하는 데에 사용)
    - 스트리밍형 : 차례차례로 생성되는 데이터를 끊임없이 계속해서 보내는 방법. (모바일 애플리케이션과 임베디드 장비 등에서 널리 데이터를 수집하는 데 사용)

2. 스트림 처리, 배치 처리 :
    - 스트림 처리(stream processing) : 스트리밍형으로 받은 데이터는 아무래도 **실시간**으로 처리하고 싶으니 스트림 처리. 
        - 시계열 DB 와 같은 실시간 처리를 지향한 데이터베이스가 자주 사용됨. 스트림 처리 결과를 이 시계열 DB에 저장함으로써 현재 무슨 일이 일어나고 있는지 즉시 파악 가능
        - 단, 장기적인 데이터 분석에는 적합하지 않을 수 있음. (지난 1년 간의 데이터 분석하려고 하면 데이터양 단번에 수만배로 증가..) 이럴땐 배치 처리 구조가 유리.
    - 배치 처리(batch processing) : **대량**의 데이터를 저장하고 처리하는 데 적합한 분산 시스템. 즉, **어느 정도 정리된 데이터**를 효율적으로 가공하기 위한 구조. 

3. 분산 스토리지(distribute storage) : 수집된 데이터가 저장되는 곳. 여러 컴퓨터와 디스크로부터 구성된 storage system. 데이터를 저장하는 방법에는 몇 가지 방법이 있는데, 가장 대표적인 것이 ‘객체 스토리지(object storage)’로, 한 덩어리로 모인 데이터에 이름을 부여해 파일로 저장하는 방법. (예. 클라우드 서비스인 Amazon S3)

4. 분산 데이터 처리(distribute data processing) : 분산 스토리지에 저장된 데이터를 처리하는 데에는 ‘분산 데이터 처리'의 프레임워크가 필요함. 주 역할은, 나중에 분석하기 쉽도록 데이터를 가공해서 그 결과를 외부 데이터베이스에 저장하는 것. 
    - 대부분 사람들은 데이터 집계하는 데 SQL 을 사용하는 데에 익숙하므로, 빅데이터를 SQL 로 집계하고 자 할때는 두 가지 방법이 있음. 
        - 분산 스토리지 상의 데이터를 SQL 로 집계하기 위해 ‘쿼리 엔진(query engine)’을 도입하는 것. (예. Hive, Interactive query engine)
        - 외부의 데이터웨어하우스 제품을 이용하는 것. → 이를 위해서는 분산 스토리지에서 추출한 데이터를 데이터 웨어하우스에 적합한 형식으로 변환해야 하고, 이 절차를 ETL 프로세스(Extract 데이터 추출 -transform 데이터 가공 -load 웨어하우스에 로드) 라고 한다. 

5. 워크플로 관리(workflow management) : 전체 데이터 파이프라인의 동작을 관리하기 위해 ‘워크플로 관리’ 기술 사용. 매일 정해진 시간에 배치 처리를 스케줄대로 실행하고, 오류가 발생하면 관리자에게 통지하는 목적으로 사용. 빅데이터 처리 시에는 크건 작건 시스템 장애가 발생하므로 오류 발생 시의 처리와 재실행을 위한 기능 제작은 필수임.

⇒ 빅데이터의 파이프 라인을 실현하려면 많은 기술과 소프트웨어가 사용된다. 전부 필수는 아니지만, 더 좋은 데이터 분석 환경을 구축하기 위해서는 각각의 특징들을 이해할 필요 있음.

⇒ 데이터 파이프라인의 큰 흐름은 변하지 않는다. 따라서, 빅데이터를 다루는 도구 선택의 경우 어떤 것을 선택해도 실제로 하는 일은 크게 다르지 않다. 1) 저장할 수 있는 데이터 용량에 제한이 없는지 2) 데이터를 효율적으로 추출할 수단이 있는지 이 2가지를 가지고 도구를 선택하면 됨.

## 데이터를 수집하는 목적

: 데이터를 수집한 후, 무엇을 실시할지는 “목적"에 따라 달라짐.

1. 데이터 검색이 목적이라면, 필요할 때 신속하게 검색할 수 있도록 해야함. 따라서, 시스템에는 실시간 데이터 처리나 검색 엔진을 활용해 키워드를 찾는 기능이 필요할 것. (예. 시스템 장애 시 그 원인 확인 위해 시스템 로그, 고객의 행동 이력 등을 확인할 때. 이 경우 데이터를 최대한 많이 취득해놔야겠지.) → 수동
2. 데이터 가공이 목적이라면, 목적이 명확하기 때문에 필요한 데이터를 계획적으로 모아 데이터 파이프 라인 설계해야 함. (예. 웹사이트에서 추천 상품 제안하거나, 센서 데이터의 비정상적인 상태 감시해 통보할 때.) → 데이터 가공에는 자동화가 필수적. 따라서, 워크플로 관리 도입해 꼼꼼히 테스트를 반복적으러 실행해 시스템 구축.
3. 데이터 시각화가 목적이라면, 확실한 해답은 없음. 임의의 분석 환경을 갖추고 여러번 데이터 집계를 반복함. 시각화를 고속화 하려면 데이터 마트로 필요함. (* 데이터 마트란? 수집한 로우 데이터를 그대로 보존하는 데이터 레이크에서, 분석용으로 일부를 추출한 데이터. 즉, 데이터 레이크가 데이터를 그대로 축적시키는 곳이라면, 데이터 마트에는 분석용으로 사용될 데이터를 따로 가공해 정리함. 그리고 그것을 BI 도구 등을 통해 액세스하고, 원하는 때에 원하는 정보를 얻을 수 있도록 함.)

## 데이터 분석

1. 확증적 데이터 분석(confirmatory data analysis) : 가설을 세우고 검증. 주로 통계학적인 모델링 (통계 분석, 머신러닝) → 이 분야는 이 책의 범위를 넘음. 다루지 않을 것. 이 책의 목적은 데이터 파이프라인을 자동화하는 부분까지임.
2. 탐색적 데이터 분석(exploratory data analysis) : 데이터 시각화한 것을 보고 그 의미를 인간이 읽어내는 것. → 이 책에서는 탐색적 데이터 분석을 위하여, 대화식으로 데이터를 집계하여 시각화하기 위한 환경을 만들 것.(파이썬, R과 같은 스크립트 언어를 사용한 데이터 처리, BI 도구를 사용한 데이터 디스커버리 등)

## 스몰데이터 탐색

스몰데이터 : 사내에서 작성된 엑셀(스프레드시트) 파일, 웹에서 다운로드한 CSV 파일 등…. 스몰 데이터 탐색은 분산 시스템을 전혀 사용하지 않는 한 대의 컴퓨터로 데이터를 탐색하는 것으로, 우리가 해결하고자 하는 문제가 스몰 데이터 기술로 충분하다면 그 방식으로 탐색하는 것이 효과적이므로 빅데이터 탐색 기술을 알기 전에 반드시 알고 있어야 함.

스프레드시트에 의한 보고서 작성은 그 한계점이 명확함. 

1. 보고서에 입력하는 숫자를 어디선가 계산해야 한다는 것. → 이를 위해 데이터 웨어하우스 등장. 여기서 실행되는 것이 배치 처리이고 이것은 자동화될 수 있음.
2. 상세한 내역을 조사하는 것이 어려움. → 이를 위해 BI 도구 사용. (예. Tableaus Public, Quick Sencse, Microsoft Power BI, Google data studio) BI 도구는 고속의 집계 엔진을 내장하고 있어 수백만 레코드 정도의 스몰 데이터라면 순식간에 그래프 그려주기 때문에, 애드 혹 분석(ad hoc analysis)에서 대화형으로 데이터를 시각화하고 싶을 때 특히 편리함. 

*애드 혹 분석이란? 자동화를 생각하지 않고 수작업으로 데이터를 집계하여 분석하는 것. 일회성 데이터 분석이라는 의미. SQL 쿼리를 직접 작성해서 실행하거나 스프레드시트 에서 그래프를 만드는 것 까지 포함해 모든 수작업이 애드 혹 분석에 포함됨.

*BI 도구를 위한 테이블을 처음부터 설계하기 보다는 우선 수동으로 입력하는 것이 쉬울 것. 이후, 자주 업데이트 되는 데이터와 다수의 사람에게 공유되는 데이터 등 중요성이 높은 것은 차례로 자동화해 나간다. 자동화하려는 경우에는 데이터 마트를 만든다. 


